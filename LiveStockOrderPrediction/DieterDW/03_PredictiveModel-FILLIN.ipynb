{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vector/matrix library\n",
    "import numpy as np\n",
    "\n",
    "#data frame library (similar to R)\n",
    "import pandas as pd\n",
    "\n",
    "#visualization library\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#regular expression library for data cleasning\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_full_dataset = <FILL_IN>L\n",
    "df_full = pd.read_csv(path_full_dataset, sep='\\t')\n",
    "\n",
    "#check the data types for the columns and cast if necessary (type information gets lost during serialization)\n",
    "<FILL_IN>\n",
    "\n",
    "print(df_full.dtypes)\n",
    "\n",
    "#read test\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Merging train and testdata\n",
    "\n",
    "* We will create a matrix representation with the dataset we have create so far. As the transformations on the data will be the same for both train and test data merge the two sets together. Note that the testset has in 'id' column which will be NaN for the training data (that's how you can separate them at the end)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = <FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Simple Linear Regression Model\n",
    "\n",
    "#### X: features\n",
    "\n",
    "* weekend: boolean value extracted from day of the week\n",
    "* quarter: extracted from month (first 3 months are Q1,... last 3 are Q4)\n",
    "* province: can be extracted from the province\n",
    "* animal_type\n",
    "* day_offset: can be caculated as the number of days since first order)\n",
    "\n",
    "#### y: sum of quantities ordered for a certain combination of features\n",
    "\n",
    "\n",
    "## A. Creating the training/test matrix\n",
    "\n",
    "\n",
    "\n",
    "Machine learning models can only deal with numerical data. Now we will map our dataframe onto a numerical respresentation (i.e. numpy arrays). \n",
    "\n",
    "From now on we scikit-learn will be our best friend. This is the defacto library for training models in python. Other specialized libraries exist, for example focusing on neural networks, but this will often be your first resort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn follows a very simple approach: http://scikit-learn.org/stable/data_transforms.html\n",
    "\n",
    "- **Estimators* have a fit method, which learns model parameters from a dataset\n",
    "- *Transformers** have a transform method which applies this transformation model to unseen data. Making a prediction is considered a transformation, but also scaling the data for example!\n",
    "- fit_transform combines the above\n",
    "\n",
    "Two important data preparation steps are:\n",
    "\n",
    "- Continuous variables should be **scaled** (map all values to [-1,+1]), this will improve the performance of most learning algorithms (sklearn minmaxscaler)\n",
    "\n",
    "- Categorical data should be mapped on a binary scale {0,1} (sklearn labelencoder). If you have more than two categories the recommended technique for mapping is called **one-hot-encoding** (pandas routine get_dummies can help you with that)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#pd.get_dummies(series, prefix='...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. day_offset is a continuous variable which could be mapped on [0,1]\n",
    "\n",
    "* convert using the MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. One-hot-encoding\n",
    "\n",
    "* `pd.get_dummies(series)`\n",
    "* OHE for categorical data: animal types, province\n",
    "\n",
    "**HINT1** As there are a lot of additional columns created during OHE, it can get difficult to not lose track where the variables originally came from. The get_dummies function allows you to prefix the new column names. Here you could use 'animal' for example. Then you'd get columns: animal_chicken, animal_cow,...\n",
    "\n",
    "**HINT2** The output of the OHE call is again a dataframe. Merge with the original frame using pandas concat function:\n",
    "\n",
    "\n",
    "\n",
    "`pd.concat([df1, df2], axis=1)` \n",
    "\n",
    "**NOTE** If you only have a binary categorical variable you can also use `labelEncoder`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Additional features can be calculated...\n",
    "\n",
    "- 'weekend' is a boolean variable which can be easily extracted from 'weekday'\n",
    "- 'quarter' can be easily extracted from 'month'\n",
    "\n",
    "**HINT:** Simply use pandas to create new columns via the following template:\n",
    "`df['columname'].apply(lambda x: f(x))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split training and test data based on the presence/absence of a value in the id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = <FILL_IN>\n",
    "df_test = <FILL_IN>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convert your dataframes to numpy matrices via `df.values` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = <FILL_IN>\n",
    "y_train = <FILL_IN>\n",
    "\n",
    "X_test = <FILL_IN>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* initialize a linear model from sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lin_reg = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit your model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With this model we can now predict the weights on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predict = lin_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Merge your predictions with the df_test dataframe and prepare a Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['quantity_ordered'] = y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test[['id', 'quantity_ordered']].to_csv('./submissions/LR1.csv', sep=',', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ...Make your first Kaggle Submission and prepare for GLORLY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Offline Model Evaluation\n",
    "\n",
    "### Train / Test - Cross Validations - or?\n",
    "\n",
    "* In a machine learning task the part of the data is typically used for training the data (training set) and part of the data is used for offline evaluation (test set). \n",
    "\n",
    "* An even better approach is to opt for <a href=\"http://scikit-learn.org/stable/modules/cross_validation.html\"> K-fold cross validation </a>. In the case you divide your data in K equal fractions. K-1 fractions are used for training and 1 is used for evaluation, this can be repeated K-times.\n",
    "\n",
    "* Depending on the nature of the feature you used it might be necessary to make a 'causal' split of the data. In this case your training data is all the orders before a date T, the testset is then the days after T. This might have very different behaviour, but might in fact be the most realistic choice for a forecasting model. Further reading about **data leakage: ** https://www.kaggle.com/wiki/Leakage\n",
    "\n",
    "### Hands-first approach\n",
    "\n",
    "* While you are used to having an in-depth theoretical coverage prior to doing something hands-on, I personally think it is quite interesting to actually flip this and treat the algorithms as black boxes and to learn more about them as you go. As a first source of what is available I would suggest to take a tour of supervised methods starting here: http://scikit-learn.org/stable/supervised_learning.html . Do focus on the regression algorithms as we are trying to predict a continuous variable. A higher level overview of some popular ML algorithms, read for example this post on KDNuggest: https://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html/2\n",
    "\n",
    "### Some algorithms...\n",
    "\n",
    "\n",
    "* Basic algorithms which I recommend you to play with are:\n",
    "\n",
    "    - <a href=\"http://scikit-learn.org/stable/modules/linear_model.html\"> Linear Regression </a>\n",
    "    - <a href=\"http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-regression\"> Nearest Neighbours </a>\n",
    "    - <a href=\"http://scikit-learn.org/stable/modules/svm.html#regression\"> SVMs </a>\n",
    "    - <a href=\"http://scikit-learn.org/stable/modules/tree.html#regression\"> Decision Trees </a>\n",
    "    - <a href=\"http://scikit-learn.org/stable/modules/ensemble.html#random-forests\"> Random Forests </a>: very important here is that RFs can be used for feature selection, you can ask them what are the features of highest importance.\n",
    "    - <a href=\"http://scikit-learn.org/stable/modules/ensemble.html#\"> Ensemble Methods </a> (example: creating your own ensemble where each algorithm gets voting rights)\n",
    "\n",
    "### Diagnostics: Learning curves\n",
    "\n",
    "* Learning curves show the error on the training vs test data as a function of the training set size. These curves give you information on whether your model has too many parameters and is overfitting (small training error but large test error) or your model is too simple  (large training error = large test error). A blog post which visualizes this concept: https://www.dataquest.io/blog/learning-curves-machine-learning/\n",
    "\n",
    "* sklearn page: http://scikit-learn.org/stable/modules/learning_curve.html\n",
    "\n",
    "* sklearn page about model evaluation: http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "### And don't hesitate to discuss, ask questions, both on minerva and on Kaggle. \n",
    "\n",
    "* If you have questions I can invite Geert Jacobs to the Kaggle forum, you can also contact him directly (he told me he doesn't mind => geert.jacobs@actemium.com)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how to import these...\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#algorithms\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#model evaluation\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
