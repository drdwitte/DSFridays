{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vector/matrix library\n",
    "import numpy as np\n",
    "\n",
    "#data frame library (similar to R)\n",
    "import pandas as pd\n",
    "\n",
    "#visualization library\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#regular expression library for data cleasning\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Customers\n",
    "\n",
    "#### 1. Create a dataframe of the customer data\n",
    "\n",
    "Fields in the Data Warehouse: (missing in csv)\n",
    "* customer_id\n",
    "* zipcode\n",
    "* country\n",
    "* salesrep_id\n",
    "* closing_day\n",
    "* creation_date\n",
    "\n",
    "HINT: The CSV file dumps use UTF-16 character encoding, ignoring this will make the file unreadable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers = pd.read_csv(<FILL_IN>, names=colnames)\n",
    "df_customers.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Dataset characteristics\n",
    "\n",
    "1. Try to get a general impression about the data, what are the number of records, what are their ranges,...\n",
    "2. Are there duplicates in the data (customer_id), merge them appropriately!\n",
    "3. Missing data: are there NaN values in the data? \n",
    "4. How many different countries are covered in the dataset?\n",
    "5. Zipcodes: data quality is not very good: Clean the data using regular expressions!\n",
    "6. The dataframe is only fully cleaned once the datatypes have been set: (cast 'em all...)\n",
    "    - zipcode is an integer\n",
    "    - creation_date is a datetime object\n",
    "\n",
    "\n",
    "- HINT1: pd.isnull()\n",
    "- HINT2: .drop() function, keep in mind that DFs are immutable (new DF created), avoid using inplace=True as it is not idempotent.\n",
    "- HINT3: pandas has a duplicated() function\n",
    "- HINT4: describe() function gives you a dataframe summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 General Impression...\n",
    "print(<FILL_IN> + \" customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranges, average, medians,...\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Duplicates\n",
    "\n",
    "#how many duplicated records?\n",
    "<FILL_IN>\n",
    "\n",
    "#inspect near-duplicate records\n",
    "<FILL_IN>\n",
    "\n",
    "#select or merge the best records and drop the redundant rows\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Missing Data\n",
    "\n",
    "#How many rows with NaN values?\n",
    "<FILL_IN>\n",
    "\n",
    "#Inspect rows where NaNs occur\n",
    "<FILL_IN>\n",
    "\n",
    "#Deal with NaNs in a way that seems appropriate to you, is the data essential?\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Different countries in dataset?\n",
    "\n",
    "#how many customers per country?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#5 clean the zipcode with regex \n",
    "\n",
    "def cleanZip(z):\n",
    "    <FILL_IN>\n",
    "\n",
    "\n",
    "#RECOMMENDED: Test your cleanZip function on a number of typical examples!\n",
    "    \n",
    "\n",
    "#create a cleanup up zipcode column\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#6 casting to datetime and int\n",
    "\n",
    "<FILL_IN>\n",
    "\n",
    "#use the dataframe.dtypes function to verify your casts (print the results)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory \"./cleaned\" and write your cleaned csv file to disk on this location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the number of records in the input dataset and the amount remaining after cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Orders\n",
    "\n",
    "#### 1. Create a dataframe of the customer data\n",
    "\n",
    "Fields in the Data Warehouse: (missing in csv)\n",
    "* sales_id (= product identifier)\n",
    "* sb_code (can be removed)\n",
    "* packaging\n",
    "* quantity_ordered (kilograms)\n",
    "* quantity_delivered\n",
    "* date_delivery\n",
    "* date_order\n",
    "* customer_id\n",
    "* silo_id\n",
    "* status_code (97 order canceled, other status codes are OK)\n",
    "\n",
    "\n",
    "HINT: The CSV file dumps use UTF-16 character encoding, ignoring this will make the file unreadable\n",
    "\n",
    "### Clean this data the same way you did the customer data and write orders_cleaned.csv to disk\n",
    "\n",
    "1. How many records and how many features are in train and test sets?\n",
    "2. Are there duplicates in the data?\n",
    "3. Missing data: are there NaN values in the data. \n",
    "4. The dataframe is only fully cleaned once the datatypes have been set: \n",
    "    - dates => datetime64\n",
    "    - ids => integer \n",
    "    - quantities => integer\n",
    "    \n",
    "5. Data Quality analysis delivery quantities is recommended (Analysis of outliers)\n",
    "    - There is a file with internal customers, their orders should be removed!\n",
    "6. Data Quality analysis on dates is recommended (Analysis of outliers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 read dataframe and clean columns\n",
    "\n",
    "df_orders = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 how many records\n",
    "print(\"Number of rows: \" + <FILL_IN>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranges, average, medians,...\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of the records with a bad status code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 duplicates \n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 missing data\n",
    "<FILL_IN>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 casting\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#5 Outlier analysis / data quality for quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write down your observations\n",
    "<FILL_IN>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with internal customers\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders12.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#6 Outlier analysis / data quality for datetimes\n",
    "<FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your cleaned csv file to disk on \"./cleaned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Here we will try to create one big dataframe (denormalization) which will serve as the basis for later modeling.\n",
    "\n",
    "Link sales and food type + food families\n",
    "\n",
    "1. melkkoe = dairy_cow\n",
    "2. vleeskoe = meat_cow\n",
    "3. varken = pig\n",
    "4. vleeskip = broiler_chicken\n",
    "5. legkip = laying_hen\n",
    "6. kalkoen = turkey\n",
    "7. paard = horse\n",
    "8. knaagdieren = rodent\n",
    "9. schapen = sheep\n",
    "\n",
    "**REMARK:** Type 16: internal orders for circuit cleansing => should not be in there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A animal groups\n",
    "\n",
    "* Have a look at families.csv and create a cleaned up dataframe of the form:\n",
    "\n",
    "\n",
    "| animal_group  | animal_type  | description  | \n",
    "|---------------|--------------|--------------|\n",
    "| 3             | dairy_cow    | plantaardig meel melkvee |\n",
    "| 8             | horse        | paard       |\n",
    "| 30            | broiler_chicken | vleeskuikens plantaardig spec |\n",
    "| 31            | meat_cow     | vleesvee |\n",
    "\n",
    "\n",
    "You can do this by hand, or by string matching (regex). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you cannot open it with pandas use the following approach to read the file:\n",
    "\n",
    "with open(path) as f:\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        \n",
    "\n",
    "df_groups = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add two additional rows for 'circuit_cleansing' and 'no_animal' \n",
    "#(this will simplify analysis later on, id of no_animal could be -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your cleaned csv file to disk on \"./cleaned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Production to group link\n",
    "\n",
    "* link productieartikel met productgroepV2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prod_to_grp = <FILL_IN>\n",
    "\n",
    "#clean the data the same way you did with the previous datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join this dataframe with df_groups and verify the join was successful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your cleaned csv file to disk on \"./cleaned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Sales to Production link\n",
    "\n",
    "* link salesartikel met productieartikelV3.csv\n",
    "* join with df_prod_to_grp\n",
    "* data quality: for packaging only 'S' is valid in the joined data\n",
    "\n",
    "**TIP:** Always check the number of records prior and after joining two datasets. Did you use the right join?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sales2prod = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales2prod.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your cleaned csv file to disk on \"./cleaned\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Merge with orders\n",
    "\n",
    "* Here it is very important that the joined dataset doesn't loose any orders!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your cleaned csv file to disk on \"./cleaned\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### E. Merge with customers and write your denormalized dataframe to disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
